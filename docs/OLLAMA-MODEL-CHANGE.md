# üîÑ Ollama Model Wechsel-Anleitung

**Wie √§ndert man das Ollama-Modell auf dem Raspberry Pi?**

Diese Anleitung geht davon aus, dass **llama3.2:latest bereits l√§uft**.

---

## ‚ö° Quick Guide (5 Minuten)

```bash
# 1. SSH zum Pi
ssh pi

# 2. Neues Modell herunterladen
ollama pull <model-name>

# 3. .env bearbeiten
cd ~/kitchenhelper-ai
nano .env

# √Ñndere Zeile:
# OLLAMA_MODEL=llama3.2
# zu:
# OLLAMA_MODEL=<neues-modell>

# 4. Docker neu starten
docker compose restart

# 5. Logs pr√ºfen
docker compose logs -f
```

**Fertig!** Das neue Modell wird jetzt verwendet.

---

## üìã Verf√ºgbare Modelle

### Empfohlen f√ºr Pi 5 (8GB RAM)

| Modell | Gr√∂√üe | Speed | Qualit√§t | Empfehlung |
|--------|-------|-------|----------|------------|
| **llama3.2:latest** (3B) | 2.0 GB | 4-5 tok/s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ **Standard** |
| llama3.2:1b | 1.3 GB | 7-8 tok/s | ‚≠ê‚≠ê | ‚ùå Halluzinationen |
| gemma2:2b | 1.6 GB | 5-6 tok/s | ‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Untested |
| phi3:mini | 2.3 GB | 3-4 tok/s | ‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Untested |

### F√ºr leistungsst√§rkere Hardware

| Modell | Gr√∂√üe | Speed | Qualit√§t | Hardware |
|--------|-------|-------|----------|----------|
| llama3.1:8b | 4.7 GB | 2-3 tok/s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Desktop/Server |
| mistral:7b | 4.1 GB | 2-3 tok/s | ‚≠ê‚≠ê‚≠ê‚≠ê | Desktop/Server |

---

## üîç Modell-Liste anzeigen

```bash
# Installierte Modelle
ollama list

# Verf√ºgbare Modelle (online)
ollama search llama
ollama search gemma
ollama search phi3
```

---

## üß™ Modell testen (bevor du wechselst)

```bash
# Modell herunterladen
ollama pull gemma2:2b

# Interaktiv testen
ollama run gemma2:2b

# Rezept-Prompt testen
ollama run gemma2:2b "Generate a simple recipe with tomatoes, pasta, and garlic. Return JSON format."
```

**Wenn das Ergebnis gut aussieht:** Wechsel in der `.env` durchf√ºhren.

---

## üìù Schritt-f√ºr-Schritt Anleitung

### 1Ô∏è‚É£ Neues Modell herunterladen

```bash
ssh pi
ollama pull <model-name>

# Beispiel: Gemma 2B
ollama pull gemma2:2b
```

**Download-Zeit:** 1-5 Minuten (je nach Modellgr√∂√üe)

---

### 2Ô∏è‚É£ .env Datei bearbeiten

```bash
cd ~/kitchenhelper-ai
nano .env
```

**Finde Zeile:**
```
OLLAMA_MODEL=llama3.2
```

**√Ñndere zu:**
```
OLLAMA_MODEL=gemma2:2b
```

**Speichern:** `Ctrl+O` ‚Üí `Enter` ‚Üí `Ctrl+X`

---

### 3Ô∏è‚É£ Docker neu starten

```bash
docker compose restart
```

**Oder vollst√§ndiger Neustart:**
```bash
docker compose down
docker compose up -d
```

---

### 4Ô∏è‚É£ √úberpr√ºfen

```bash
# Logs ansehen (Ctrl+C zum Beenden)
docker compose logs -f

# Erwarte:
# "Ollama available: True"
# "Ollama model: gemma2:2b"
```

**API testen:**
```bash
curl http://localhost:8000/health
# {"status":"ok"}
```

---

## üóëÔ∏è Altes Modell l√∂schen (Platz sparen)

```bash
# Liste installierte Modelle
ollama list

# L√∂sche ungenutztes Modell
ollama rm llama3.2:1b

# Best√§tigung:
# "deleted 'llama3.2:1b'"
```

**Disk-Space frei:** ~1-2 GB pro gel√∂schtes Modell

---

## ‚ö†Ô∏è Troubleshooting

### Problem: "Model not found"

```bash
# Pr√ºfe ob Modell existiert
ollama list

# Falls nicht: Herunterladen
ollama pull <model-name>
```

---

### Problem: Container startet nicht

```bash
# Logs pr√ºfen
docker compose logs

# H√§ufige Fehler:
# - "Out of memory" ‚Üí Kleineres Modell w√§hlen
# - "Connection refused" ‚Üí Ollama-Service pr√ºfen

# Ollama Service Status
sudo systemctl status ollama
```

---

### Problem: Schlechte Rezept-Qualit√§t

**M√∂gliche Ursachen:**
- Modell zu klein (< 3B Parameter)
- Temperatur zu hoch
- Prompt nicht optimal

**L√∂sung:**
- Zur√ºck zu `llama3.2:latest` (bew√§hrt)
- Gr√∂√üeres Modell testen (Desktop-Hardware)

---

## üéØ Empfohlene Modelle nach Use-Case

### Free-Tier (Pi 5)
- **llama3.2:latest** ‚Üê Beste Balance Quality/Speed

### Pro-Tier (Cloud)
- **Gemini 2.0 Flash** (API) ‚Üê 10x schneller

### Testing/Development
- **llama3.2:1b** ‚Üê Nur f√ºr Speed-Tests, NICHT Production!

---

## üìä Performance-Vergleich

| Modell | Rezept-Zeit | Qualit√§t | RAM |
|--------|-------------|----------|-----|
| llama3.2:1b | ~18s | ‚ùå Schlecht | 1.5 GB |
| llama3.2:latest | ~35s | ‚úÖ Gut | 2.5 GB |
| llama3.1:8b | ~60s | ‚≠ê Sehr gut | 5.5 GB |
| Gemini Flash | ~3s | ‚≠ê‚≠ê Exzellent | Cloud |

---

## üîÑ Zur√ºck zu Standard-Modell

```bash
cd ~/kitchenhelper-ai
nano .env

# √Ñndere zu:
OLLAMA_MODEL=llama3.2

docker compose restart
```

---

## üìö Weitere Ressourcen

- **Ollama Library:** https://ollama.com/library
- **Model Cards:** Details zu jedem Modell (Parameter, Training, etc.)
- **Pi Deployment Guide:** `docs/PI-DEPLOYMENT.md`

---

**Letzte Aktualisierung:** 2025-12-01
**Getestet mit:** Raspberry Pi 5 (8GB), Ollama 0.x, llama3.2:latest
