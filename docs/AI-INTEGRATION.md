# ğŸ¤– AI Integration - Gemini & Ollama

## Ãœbersicht

KitchenHelper-AI verwendet ein **tier-basiertes AI-System** fÃ¼r die Rezeptgenerierung:

- **Free Tier:** Ollama (lokal, kostenlos, datenschutzfreundlich)
- **Pro Tier:** Gemini Flash (schnell, Cloud) mit automatischem Ollama-Fallback

## Architektur

```
User Request
    â†“
Recipe Generator Route (/recipes/generate)
    â†“
AI Provider Selection (based on user tier)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Free Tier: â†’ Ollama (lokal)         â”‚
â”‚ Pro Tier:  â†’ Gemini + Ollama backup â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Recipe JSON Response
```

## Performance

| Tier | Provider | Generierungszeit | Kosten | Datenschutz |
|------|----------|------------------|--------|-------------|
| Free | Ollama | ~7-10s | 0â‚¬ | âœ… Lokal |
| Pro  | Gemini | ~2-3s | ~0.001â‚¬/Rezept | âš ï¸ Cloud |
| Pro  | Ollama (Fallback) | ~7-10s | 0â‚¬ | âœ… Lokal |

**User-Wahrnehmung:** 2-3x Geschwindigkeitsunterschied deutlich spÃ¼rbar!

## Setup

### 1. Ollama (fÃ¼r Free & Pro Fallback)

**Installation auf Raspberry Pi:**
```bash
# Ollama installieren
curl -fsSL https://ollama.ai/install.sh | sh

# Modell herunterladen
ollama pull llama3.2

# Server starten (lÃ¤uft automatisch als Service)
sudo systemctl start ollama
```

**Testen:**
```bash
curl http://localhost:11434/api/tags
```

### 2. Gemini API (optional, nur fÃ¼r Pro-Features)

**API Key besorgen:**
1. Gehe zu [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Erstelle einen neuen API Key
3. FÃ¼ge ihn zur `.env` hinzu:

```env
GOOGLE_AI_API_KEY=dein-api-key-hier
```

**Kosten (Stand: 2025):**
- Gemini 2.0 Flash: **KOSTENLOS** bis 15 requests/minute
- Bei mehr: $0.00075 per 1K characters (~0.001â‚¬ pro Rezept)

## Konfiguration

**Backend `.env`:**
```env
# Gemini (Pro users)
GOOGLE_AI_API_KEY=your-api-key-here
GEMINI_MODEL=gemini-2.0-flash-exp

# Ollama (Free users + Pro fallback)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

**Modell-Optionen:**

| Provider | Modell | Empfehlung |
|----------|--------|------------|
| Gemini | `gemini-2.0-flash-exp` | âœ… Schnell, kostenlos |
| Gemini | `gemini-1.5-flash` | Stabil, leicht teurer |
| Ollama | `llama3.2` | âœ… Gut, schnell (3B params) |
| Ollama | `gemma2` | Alternative, kleiner |

## API Usage

**Frontend Request:**
```javascript
const response = await fetch('/api/recipes/generate', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${token}`
  },
  body: JSON.stringify({
    ingredient_ids: [1, 2, 3],
    ai_provider: "ai",  // "mock" oder "ai"
    servings: 2,
    language: "de"
  })
});
```

**Backend Logic:**
```python
# Free User â†’ Ollama
if user_tier == "free":
    recipes = ollama.generate(...)

# Pro User â†’ Gemini mit Fallback
if user_tier == "pro":
    try:
        recipes = gemini.generate(...)
    except:
        recipes = ollama.generate(...)  # Fallback
```

## Monitoring & Logging

**Backend Logs zeigen AI Provider:**
```
INFO: AI Generator initialized - Gemini: True, Ollama: True
INFO: Pro user - Using Gemini API
INFO: Free user - Using Ollama
ERROR: Gemini failed: API timeout - Falling back to Ollama
```

**Recipe Response enthÃ¤lt Provider-Info:**
```json
{
  "recipes": [
    {
      "name": "Pasta Carbonara",
      "ai_provider": "gemini",  // oder "ollama" oder "mock"
      ...
    }
  ]
}
```

## Fehlerbehandlung

**Szenarien:**

| Fall | Pro User | Free User |
|------|----------|-----------|
| Ollama down | âŒ Error (Gemini + Fallback fail) | âŒ Error |
| Gemini down | âœ… Fallback zu Ollama | N/A |
| Beide down | âŒ Error | âŒ Error |
| Mock | âœ… Immer verfÃ¼gbar | âœ… Immer verfÃ¼gbar |

**User Feedback:**
```javascript
// Bei Fehler: Fallback zu Mock vorschlagen
if (error.status === 503) {
  alert("AI-Generierung nicht verfÃ¼gbar. Template-basierte Rezepte werden verwendet.");
  // Retry mit ai_provider="mock"
}
```

## Upgrade-Path

**Free â†’ Pro Marketing:**
- "ğŸš€ 3x schnellere Rezept-Generierung mit Pro!"
- "âš¡ Upgrade fÃ¼r sofortige Ergebnisse (2-3s statt 10s)"
- "Premium AI-Modelle mit besserer KreativitÃ¤t"

## Sicherheit

**Wichtig:**
- âœ… API Keys **niemals** im Frontend!
- âœ… API Keys nur in `.env` (nicht in Git!)
- âœ… Rate Limiting auf Backend-Seite
- âœ… User Tier aus JWT Token (nicht aus Request-Body)

**Rate Limiting:**
```python
# Gemini Free Tier Limit: 15 req/min
# â†’ Max 15 Pro-User gleichzeitig
# â†’ Bei mehr: 429 Error â†’ Ollama Fallback
```

## Zukunft

**MÃ¶gliche Erweiterungen:**
- [ ] Streaming Responses (Server-Sent Events)
- [ ] Custom Prompt Templates (User-definiert)
- [ ] Recipe Variations ("Mach es veganer")
- [ ] Multi-Language AI (Prompt-Optimization per Sprache)
- [ ] Image Generation (Rezeptfotos via DALL-E/Stable Diffusion)

---

**Implementiert:** 30.11.2025
**Status:** âœ… Production-Ready (Ollama required, Gemini optional)
