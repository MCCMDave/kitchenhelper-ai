# üöÄ Deployment Update - 01.12.2025

**Updates f√ºr Raspberry Pi Deployment**

---

## ‚ú® Neue Features

### 1. **Timeout erh√∂ht (60s ‚Üí 180s)**
- Ollama bekommt mehr Zeit f√ºr Rezept-Generierung
- Verhindert Timeouts bei langsamen Modellen
- **Datei:** `backend/app/services/ai_recipe_generator.py:173,225`

### 2. **Rate Limiting implementiert**
- Max 2-3 gleichzeitige AI-Requests
- Sch√ºtzt Pi vor √úberlastung
- Warteschlange f√ºr weitere Requests
- **Dateien:**
  - `backend/app/middleware/rate_limit.py` (neu)
  - `backend/app/main.py:42`

### 3. **FAQ-Feature**
- 10 vorgefertigte Rezept-Kategorien
- Schneller Zugriff auf h√§ufige Anfragen
- Bilingual (DE/EN)
- **Dateien:**
  - `backend/app/data/faq_recipes.py` (neu)
  - `backend/app/routes/faq.py` (neu)
  - `backend/app/main.py:57`

### 4. **Dokumentation**
- Modell-Wechsel-Anleitung
- .env Kommentare f√ºr Pi
- **Dateien:**
  - `docs/OLLAMA-MODEL-CHANGE.md` (neu)
  - `.env.example:32-37`

---

## üìã Deployment auf Pi - Schritt f√ºr Schritt

### **WICHTIG: Zuerst Updates auf Pi deployen, dann .env anpassen!**

---

### 1Ô∏è‚É£ SSH zum Pi

```bash
ssh pi
```

---

### 2Ô∏è‚É£ Git Pull (Updates holen)

```bash
cd ~/kitchenhelper-ai
git pull
```

**Erwarte:**
```
remote: Enumerating objects: 15, done.
remote: Counting objects: 100% (15/15), done.
...
Updating abc1234..def5678
Fast-forward
 backend/app/services/ai_recipe_generator.py | 4 ++--
 backend/app/middleware/rate_limit.py       | 95 ++++++++++++++++++++++++++++
 backend/app/routes/faq.py                  | 135 +++++++++++++++++++++++++++++++++++++++
 ...
```

---

### 3Ô∏è‚É£ .env anpassen (OLLAMA_BASE_URL Fix)

```bash
nano .env
```

**Finde Zeile:**
```
OLLAMA_BASE_URL=http://host.docker.internal:11434
```

**√Ñndere zu:**
```
OLLAMA_BASE_URL=http://localhost:11434
```

**Speichern:** `Ctrl+O` ‚Üí `Enter` ‚Üí `Ctrl+X`

**Warum?**
- `host.docker.internal` funktioniert nicht mit `network_mode: host`
- `localhost` greift direkt auf Pi-Host Ollama zu

---

### 4Ô∏è‚É£ Docker neu starten

```bash
docker compose down
docker compose up -d
```

**Erwarte:**
```
[+] Running 1/1
 ‚†ø Container kitchenhelper-api  Started
```

---

### 5Ô∏è‚É£ Logs pr√ºfen (wichtig!)

```bash
docker compose logs -f
```

**Erwarte:**
```
kitchenhelper-api | [OK] KitchenHelper-AI v1.0.0 started!
kitchenhelper-api | INFO:app.services.ai_recipe_generator:AI Generator initialized - Gemini: False, Ollama: True
kitchenhelper-api | INFO:app.middleware.rate_limit:OllamaRateLimiter initialized (max_concurrent: 2)
```

**‚ùå KEIN "Ollama not available" Fehler mehr!**

`Ctrl+C` zum Beenden

---

### 6Ô∏è‚É£ Health Check

```bash
curl http://localhost:8000/health
```

**Erwarte:**
```json
{"status":"ok"}
```

---

### 7Ô∏è‚É£ FAQ-Feature testen (optional)

```bash
curl http://localhost:8000/api/faq/categories \
  -H "Authorization: Bearer <dein-token>"
```

**Erwarte:**
```json
{
  "categories": [
    {"id": "quick-dinner-veg", "title": "Quick Vegetarian Dinner", ...},
    {"id": "diabetic-breakfast", "title": "Diabetic-Friendly Breakfast", ...},
    ...
  ],
  "count": 10
}
```

---

## ‚úÖ Deployment erfolgreich!

**Neue Features aktiv:**
- ‚úÖ Timeout 180s (3 Min)
- ‚úÖ Rate Limiting (max 2 concurrent)
- ‚úÖ FAQ-Feature (10 Kategorien)
- ‚úÖ Ollama Connection Fix

---

## üîß Troubleshooting

### Problem: "Ollama not available"

```bash
# 1. Pr√ºfe Ollama Service
sudo systemctl status ollama

# 2. Falls gestoppt:
sudo systemctl start ollama

# 3. Teste manuell
curl http://localhost:11434/api/tags

# 4. Docker neu starten
docker compose restart
```

---

### Problem: Rate Limit zu restriktiv

**.env bearbeiten:**
```bash
nano backend/app/middleware/rate_limit.py

# Zeile 22 √§ndern:
# max_concurrent: int = 2  ‚Üí  max_concurrent: int = 3
```

**Docker neu starten:**
```bash
docker compose restart
```

---

### Problem: FAQ l√§dt nicht

**Pr√ºfe Route:**
```bash
docker compose logs | grep faq

# Erwarte:
# "GET /api/faq/categories"
```

**Pr√ºfe Code:**
```bash
cat backend/app/main.py | grep faq

# Erwarte:
# from app.routes import (..., faq)
# app.include_router(faq.router, prefix="/api")
```

---

## üìä Neue API-Endpoints

### GET `/api/faq/categories`
**Liste aller FAQ-Kategorien**

**Query Params:**
- `language`: "en" oder "de"

**Response:**
```json
{
  "categories": [...],
  "count": 10
}
```

---

### POST `/api/faq/generate/{category_id}`
**Rezept aus FAQ-Kategorie generieren**

**Path Params:**
- `category_id`: "quick-dinner-veg", "diabetic-breakfast", etc.

**Query Params:**
- `language`: "en" oder "de"

**Response:**
```json
{
  "recipes": [...],
  "count": 3,
  "daily_count_remaining": 47,
  "message": "‚úÖ 3 Rezepte aus FAQ 'Quick Vegetarian Dinner' generiert!"
}
```

---

## üìö Weiterf√ºhrende Docs

- **Modell wechseln:** `docs/OLLAMA-MODEL-CHANGE.md`
- **Pi Deployment:** `docs/PI-DEPLOYMENT.md`
- **AI Integration:** `docs/AI-INTEGRATION.md`

---

**Deployment-Datum:** 2025-12-01
**Getestet:** ‚ùå Noch nicht auf Pi getestet
**N√§chster Schritt:** Git Push ‚Üí Pi Deployment ‚Üí Testing

---

## üéØ Next Steps (nach Deployment)

1. **Commit & Push zu GitHub** (Laptop)
2. **Pi Deployment** (wie oben beschrieben)
3. **Testing:**
   - Rezept-Generierung (Free + Pro)
   - FAQ-Feature testen
   - Rate Limiting testen (2+ concurrent requests)
   - Timeout testen (sollte nicht mehr bei 60s abbrechen)

4. **Optional:**
   - Pro Lite Tier evaluieren (Groq/Deepseek)
   - Trainingsdaten-Disclaimer platzieren
   - Frontend FAQ-UI implementieren

---

**Status:** ‚úÖ Code bereit f√ºr Deployment
